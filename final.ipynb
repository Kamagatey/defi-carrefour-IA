{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chargement des libraries necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement de tous le dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reads the data from CSV files named \"train_data_part_i.csv\" for all i from 1 to 10\n",
    "# and concatenates them into a single pandas DataFrame\n",
    "train_dataframes = []\n",
    "for i in tqdm(range(1, 11)):\n",
    "    train_dataframes.append(pd.read_csv(f'data-train/train_data_part_{i}.csv'))\n",
    "train_data = pd.concat(train_dataframes, ignore_index=True)\n",
    "\n",
    "# free up memory by deleting the dataframes we no longer need\n",
    "del train_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des transactions de 10 000 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "# Étape 1 : Obtenez une liste unique des clients dans train_data\n",
    "unique_clients = train_data['customer_id'].unique()\n",
    "\n",
    "# Étape 2 : Sélectionnez 10 000 clients aléatoires\n",
    "sampled_clients = random.sample(list(unique_clients), 10000)\n",
    "\n",
    "# Étape 3 : Filtrer `final_data` pour ces clients\n",
    "final_data_sampled = train_data[train_data['customer_id'].isin(sampled_clients)]\n",
    "\n",
    "# Étape 4 : Filtrer `test_data` pour les mêmes clients\n",
    "test_data_sampled = test_data[test_data['customer_id'].isin(sampled_clients)]\n",
    "\n",
    "# Optionnel : Sauvegardez les résultats dans des fichiers CSV\n",
    "final_data_sampled.to_csv(\"final_data_sample.csv\", index=False)\n",
    "test_data_sampled.to_csv(\"test_data_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du jeux de donnees reduit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=pd.read_csv(\"final_data_sample.csv\")\n",
    "final_data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data=pd.read_csv('data-train/product_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraitement de product_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprressions des colonnes qui ont plus de 50% de valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data=products_data.drop(columns=['shelf_level4','ecoscore','salt_reduced','naturality',\n",
    "                                          'product_description','lactose_free','no_added_salt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation d'une fonction permettant de remplacer les valeurs manquantes des variables categorielles par le Mode et des variables quantitatives par la median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Imputer les valeur manquantes\n",
    "import numpy as np\n",
    "\n",
    "def Imput_ValeurManquate ( Data, columns_Num, columns_Cat, Imput_Num= \"median\", \n",
    "                          Imput_Cat= \"Mode\" ) :\n",
    "    for col in columns_Num :\n",
    "        if Imput_Num == \"median\" :\n",
    "            Change = np.median(Data[col].dropna()) \n",
    "        else:  \n",
    "            Change = int(Imput_Num)\n",
    "        Data[col] = Data[col].fillna(Change)\n",
    "    for col in columns_Cat :\n",
    "        if Imput_Cat == \"Mode\" :\n",
    "            Change = str(Data[col].mode()[0])\n",
    "        else:  \n",
    "            Change = str(Imput_Cat)\n",
    "        Data[col] = Data[col].fillna(Change)\n",
    "    return(Data)    \n",
    "    \n",
    "\n",
    "# Application de la fonction sur les colonnes 'brand_key','shelf_level3' qui ont prés de 1% de valeurs manquantes\n",
    "products_data = Imput_ValeurManquate(products_data, [\"alcool\"], ['brand_key','shelf_level3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jointure du train_data et de product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fusion des données sur product_id\n",
    "final_data = pd.merge(train_data, products_data, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation des features important du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule le nombre total de transactions par client\n",
    "final_data['total_transactions'] = final_data.groupby('customer_id')['transaction_id'].transform('count')\n",
    "\n",
    "# Calcule le nombre total de produits uniques achetés par client\n",
    "final_data['total_unique_products'] = final_data.groupby('customer_id')['product_id'].transform('nunique')\n",
    "\n",
    "# Calcule l'utilisation moyenne de la carte de fidélité par client\n",
    "# Moyenne des valeurs de la colonne 'has_loyality_card' pour chaque client\n",
    "final_data['loyalty_card_usage'] = final_data.groupby('customer_id')['has_loyality_card'].transform('mean')\n",
    "\n",
    "# Calcule le temps écoulé depuis le dernier achat pour chaque client\n",
    "# Convertit les dates en format datetime, trouve la date la plus récente, puis calcule la différence avec chaque date\n",
    "final_data['time_since_last_purchase'] = final_data.groupby('customer_id')['date'].transform(\n",
    "    lambda x: (pd.to_datetime(x).max() - pd.to_datetime(x)).dt.days\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation de la variable cible pour le modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données de test\n",
    "# On charge le fichier CSV contenant les données de test reduit\n",
    "test_data = pd.read_csv('data_sample/test_data_sample.csv')\n",
    "\n",
    "# Création d'un dictionnaire contenant la liste des produits réellement achetés par chaque client\n",
    "# La clé est 'customer_id' et la valeur est une liste de 'product_id' achetés\n",
    "actual_purchases = test_data.groupby('customer_id')['product_id'].apply(list).to_dict()\n",
    "\n",
    "# Ajout de la colonne cible ('purchased_next') pour le modèle\n",
    "# Cette colonne indique si un produit donné (product_id) a été acheté par un client donné (customer_id)\n",
    "# Valeur 1 : Le produit a été acheté\n",
    "# Valeur 0 : Le produit n'a pas été acheté\n",
    "final_data['purchased_next'] = final_data.apply(\n",
    "    lambda row: 1 if row['product_id'] in actual_purchases.get(row['customer_id'], []) else 0,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la colonne 'date' en format datetime\n",
    "# Permet d'extraire facilement des informations temporelles (jour de la semaine, mois, etc.)\n",
    "final_data['date'] = pd.to_datetime(final_data['date'])\n",
    "\n",
    "# Extraction du jour de la semaine (0 = lundi, 6 = dimanche)\n",
    "final_data['day_of_week'] = final_data['date'].dt.dayofweek\n",
    "\n",
    "# Extraction du mois (1 = janvier, 12 = décembre)\n",
    "final_data['month'] = final_data['date'].dt.month\n",
    "\n",
    "# Calcul de la fréquence des achats par client\n",
    "# Regroupe par 'customer_id' et compte le nombre de transactions uniques ('transaction_id')\n",
    "customer_frequency = final_data.groupby('customer_id')['transaction_id'].nunique().reset_index()\n",
    "customer_frequency.rename(columns={'transaction_id': 'purchase_frequency'}, inplace=True)\n",
    "\n",
    "# Fusionne la fréquence des achats avec les données finales\n",
    "final_data = final_data.merge(customer_frequency, on='customer_id', how='left')\n",
    "\n",
    "# Identification du produit préféré par client\n",
    "# Somme les quantités achetées de chaque produit par client\n",
    "favorite_products = final_data.groupby(['customer_id', 'product_id'])['quantity'].sum().reset_index()\n",
    "# Trie par client et par quantité (ordre décroissant)\n",
    "favorite_products = favorite_products.sort_values(['customer_id', 'quantity'], ascending=[True, False]).groupby('customer_id').head(1)\n",
    "favorite_products.rename(columns={'product_id': 'favorite_product'}, inplace=True)\n",
    "\n",
    "# Fusionne les produits préférés avec les données finales\n",
    "final_data = final_data.merge(favorite_products[['customer_id', 'favorite_product']], on='customer_id', how='left')\n",
    "\n",
    "# Calcul de la quantité moyenne de produits achetés par transaction pour chaque client\n",
    "customer_avg_products = final_data.groupby('customer_id')['quantity'].mean().reset_index()\n",
    "customer_avg_products.rename(columns={'quantity': 'avg_products_per_transaction'}, inplace=True)\n",
    "\n",
    "# Calcul de la sensibilité aux promotions pour chaque client\n",
    "# Moyenne de la colonne 'is_promo' par client\n",
    "customer_promo_sensitivity = final_data.groupby('customer_id')['is_promo'].mean().reset_index()\n",
    "customer_promo_sensitivity.rename(columns={'is_promo': 'promo_sensitivity'}, inplace=True)\n",
    "\n",
    "# Identification du dernier produit acheté par chaque client\n",
    "# Trie les données par date, puis récupère le dernier produit acheté\n",
    "last_purchase = final_data.sort_values('date').groupby('customer_id')['product_id'].last().reset_index()\n",
    "last_purchase.rename(columns={'product_id': 'last_product'}, inplace=True)\n",
    "\n",
    "# Identification de la catégorie préférée de chaque client\n",
    "# Somme les quantités achetées par catégorie ('department_key') pour chaque client\n",
    "favorite_category = final_data.groupby(['customer_id', 'department_key'])['quantity'].sum().reset_index()\n",
    "# Trie par client et par quantité (ordre décroissant)\n",
    "favorite_category = favorite_category.sort_values(['customer_id', 'quantity'], ascending=[True, False]).groupby('customer_id').head(1)\n",
    "favorite_category.rename(columns={'department_key': 'favorite_category'}, inplace=True)\n",
    "\n",
    "# Calcul de la popularité globale des produits\n",
    "# Somme les quantités achetées pour chaque produit\n",
    "product_popularity = final_data.groupby('product_id')['quantity'].sum().reset_index()\n",
    "product_popularity.rename(columns={'quantity': 'global_popularity'}, inplace=True)\n",
    "\n",
    "# Calcul de la sensibilité aux promotions pour chaque produit\n",
    "# Moyenne de la colonne 'is_promo' par produit\n",
    "product_promo_sensitivity = final_data.groupby('product_id')['is_promo'].mean().reset_index()\n",
    "product_promo_sensitivity.rename(columns={'is_promo': 'promo_sensitivity'}, inplace=True)\n",
    "\n",
    "# Calcul du nombre d'acheteurs uniques par produit\n",
    "# Compte le nombre unique de clients ayant acheté chaque produit\n",
    "product_unique_buyers = final_data.groupby('product_id')['customer_id'].nunique().reset_index()\n",
    "product_unique_buyers.rename(columns={'customer_id': 'unique_buyers'}, inplace=True)\n",
    "\n",
    "# Exemple de fusion pour toutes les features clients\n",
    "# Fusionne les informations calculées sur les clients avec les données finales\n",
    "final_data = final_data.merge(customer_frequency, on='customer_id', how='left')  # Fréquence d'achat\n",
    "final_data = final_data.merge(customer_avg_products, on='customer_id', how='left')  # Moyenne des produits achetés par transaction\n",
    "final_data = final_data.merge(customer_promo_sensitivity, on='customer_id', how='left')  # Sensibilité aux promotions\n",
    "final_data = final_data.merge(last_purchase, on='customer_id', how='left')  # Dernier produit acheté\n",
    "# final_data = final_data.merge(favorite_products[['customer_id', 'favorite_product']], on='customer_id', how='left')  # Produit préféré\n",
    "# final_data = final_data.merge(favorite_category[['customer_id', 'favorite_category']], on='customer_id', how='left')  # Catégorie préférée\n",
    "\n",
    "# Exemple de fusion pour toutes les features produits\n",
    "# Fusionne les informations calculées sur les produits avec les données finales\n",
    "final_data = final_data.merge(product_popularity, on='product_id', how='left')  # Popularité globale\n",
    "final_data = final_data.merge(product_promo_sensitivity, on='product_id', how='left')  # Sensibilité aux promotions\n",
    "final_data = final_data.merge(product_unique_buyers, on='product_id', how='left')  # Nombre d'acheteurs uniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les modalités valides pour certaines colonnes\n",
    "# Chaque clé représente une colonne, et la valeur correspond à une liste des modalités à conserver\n",
    "valid_modalities = {\n",
    "    'format': ['DRIVE', 'CLCV'],  # Modalités valides pour la colonne 'format'\n",
    "    'order_channel': ['WEBSITE', 'MOBILE_APP'],  # Modalités valides pour la colonne 'order_channel'\n",
    "    'department_key': ['Department_25', 'Department_14', 'Department_10', 'Department_22', 'Department_12'],  # Modalités valides pour 'department_key'\n",
    "    'sector': ['PGC', 'PRODUITS FRAIS TRANSFORMATION']  # Modalités valides pour la colonne 'sector'\n",
    "}\n",
    "\n",
    "# Filtrer les colonnes du DataFrame pour ne conserver que les modalités valides\n",
    "for column, modalities in valid_modalities.items():  # Itère sur les colonnes et leurs modalités valides\n",
    "    # Remplace les valeurs qui ne sont pas dans les modalités valides par None\n",
    "    final_data[column] = final_data[column].apply(lambda x: x if x in modalities else None)\n",
    "\n",
    "# Encoder les variables catégorielles avec la méthode one-hot encoding\n",
    "# La méthode pd.get_dummies crée des colonnes binaires pour chaque modalité présente dans les colonnes spécifiées\n",
    "# Ici, les colonnes correspondant aux clés de 'valid_modalities' sont encodées\n",
    "# L'argument drop_first=False conserve toutes les modalités pour chaque colonne\n",
    "final_data = pd.get_dummies(final_data, columns=valid_modalities.keys(), drop_first=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des types de colonnes à encoder\n",
    "categorical_cols_onehot = ['day_of_week', 'month']  # Colonnes catégoriques avec un petit nombre de modalités pour One-Hot Encoding\n",
    "categorical_cols_target = [  # Colonnes catégoriques importantes à encoder via Target Encoding (ou Frequency Encoding)\n",
    "    'class_key', 'shelf_level1', 'last_product', 'store_id', 'subclass_key', \n",
    "    'brand_key', 'shelf_level2', 'shelf_level3'\n",
    "]\n",
    "bool_cols = [  # Liste des colonnes booléennes représentant des caractéristiques spécifiques des produits\n",
    "    'has_loyality_card', 'is_promo', 'bio', 'sugar_free', 'aspartame_free', 'gluten_free', \n",
    "    'halal', 'casher', 'eco_friendly', 'local_french', 'artificial_coloring_free', \n",
    "    'taste_enhancer_free', 'antibiotic_free', 'reduced_sugar', 'vegetarian', 'pesticide_free', \n",
    "    'grain_free', 'no_added_sugar', 'nitrite_free', 'fed_without_ogm', 'no_artificial_flavours', \n",
    "    'porc', 'vegan', 'frozen', 'fat_free', 'reduced_fats', 'fresh', 'alcool', \n",
    "    'phenylalanine_free', 'palm_oil_free', 'produits_du_monde', 'regional_product', \n",
    "    'national_brand', 'first_price_brand', 'carrefour_brand'\n",
    "]\n",
    "\n",
    "# Étape 1 : Application du One-Hot Encoding pour les colonnes catégoriques avec peu de modalités\n",
    "# On transforme les colonnes catégoriques spécifiées dans 'categorical_cols_onehot' en variables binaires\n",
    "# Exemple : La colonne 'day_of_week' pourrait être transformée en 6 colonnes binaires représentant les jours de la semaine (sauf le premier)\n",
    "final_data = pd.get_dummies(final_data, columns=categorical_cols_onehot, drop_first=True)\n",
    "\n",
    "# Définition d'une fonction pour encoder une colonne avec les fréquences\n",
    "def frequency_encode(df, column):\n",
    "    # Calcul de la fréquence relative pour chaque modalité de la colonne\n",
    "    freq_map = df[column].value_counts(normalize=True)  # Renvoie un dictionnaire {valeur: fréquence}\n",
    "    # Remplacement des valeurs de la colonne par leurs fréquences\n",
    "    return df[column].map(freq_map)\n",
    "\n",
    "# Étape 2 : Application du Frequency Encoding sur les colonnes catégoriques ciblées\n",
    "# On encode chaque colonne dans 'categorical_cols_target' avec les fréquences des modalités\n",
    "for col in categorical_cols_target:\n",
    "    final_data[col] = frequency_encode(final_data, col)\n",
    "\n",
    "# Suppression de la colonne redondante ou incorrecte 'purchase_frequency_y'\n",
    "# Cela peut être nécessaire si cette colonne a été ajoutée lors de fusion(s) ou calcul(s) précédents\n",
    "final_data = final_data.drop(columns=['purchase_frequency_y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde du jeu de données pretraité "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('data_sample/final_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``DEBUT DE L'ENTRAINEMENT DU MODELE``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>has_loyality_card</th>\n",
       "      <th>store_id</th>\n",
       "      <th>is_promo</th>\n",
       "      <th>quantity</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_unique_products</th>\n",
       "      <th>...</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-12</td>\n",
       "      <td>Transaction_1479608</td>\n",
       "      <td>Household_167</td>\n",
       "      <td>Product_8327</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1189</td>\n",
       "      <td>287</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-18</td>\n",
       "      <td>Transaction_993002</td>\n",
       "      <td>Household_167</td>\n",
       "      <td>Product_3846</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1189</td>\n",
       "      <td>287</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>Transaction_2113977</td>\n",
       "      <td>Household_1956</td>\n",
       "      <td>Product_70955</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>772</td>\n",
       "      <td>596</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-09</td>\n",
       "      <td>Transaction_1686434</td>\n",
       "      <td>Household_1956</td>\n",
       "      <td>Product_204</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>772</td>\n",
       "      <td>596</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>Transaction_2032633</td>\n",
       "      <td>Household_1956</td>\n",
       "      <td>Product_10212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>772</td>\n",
       "      <td>596</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867119</th>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>Transaction_703715</td>\n",
       "      <td>Household_72474</td>\n",
       "      <td>Product_56965</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>532</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867120</th>\n",
       "      <td>2023-03-19</td>\n",
       "      <td>Transaction_1183149</td>\n",
       "      <td>Household_72474</td>\n",
       "      <td>Product_4110</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>532</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867121</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>Transaction_8549</td>\n",
       "      <td>Household_72474</td>\n",
       "      <td>Product_58020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>532</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867122</th>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>Transaction_1684683</td>\n",
       "      <td>Household_72474</td>\n",
       "      <td>Product_46541</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>532</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867123</th>\n",
       "      <td>2023-06-25</td>\n",
       "      <td>Transaction_2303734</td>\n",
       "      <td>Household_79007</td>\n",
       "      <td>Product_73321</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>439</td>\n",
       "      <td>205</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8867124 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       transaction_id      customer_id     product_id  \\\n",
       "0        2023-07-12  Transaction_1479608    Household_167   Product_8327   \n",
       "1        2023-07-18   Transaction_993002    Household_167   Product_3846   \n",
       "2        2023-03-06  Transaction_2113977   Household_1956  Product_70955   \n",
       "3        2022-02-09  Transaction_1686434   Household_1956    Product_204   \n",
       "4        2023-01-23  Transaction_2032633   Household_1956  Product_10212   \n",
       "...             ...                  ...              ...            ...   \n",
       "8867119  2023-06-28   Transaction_703715  Household_72474  Product_56965   \n",
       "8867120  2023-03-19  Transaction_1183149  Household_72474   Product_4110   \n",
       "8867121  2023-02-28     Transaction_8549  Household_72474  Product_58020   \n",
       "8867122  2023-03-30  Transaction_1684683  Household_72474  Product_46541   \n",
       "8867123  2023-06-25  Transaction_2303734  Household_79007  Product_73321   \n",
       "\n",
       "         has_loyality_card  store_id  is_promo  quantity  total_transactions  \\\n",
       "0                        0  0.000452         0       1.0                1189   \n",
       "1                        0  0.000452         0       2.0                1189   \n",
       "2                        0  0.000452         0       1.0                 772   \n",
       "3                        0  0.000452         0       1.0                 772   \n",
       "4                        0  0.000452         1       2.0                 772   \n",
       "...                    ...       ...       ...       ...                 ...   \n",
       "8867119                  0  0.000636         0       2.0                 532   \n",
       "8867120                  0  0.000636         0       1.0                 532   \n",
       "8867121                  0  0.000636         0       1.0                 532   \n",
       "8867122                  0  0.000636         0       1.0                 532   \n",
       "8867123                  0  0.000155         0       1.0                 439   \n",
       "\n",
       "         total_unique_products  ...  month_3  month_4  month_5  month_6  \\\n",
       "0                          287  ...    False    False    False    False   \n",
       "1                          287  ...    False    False    False    False   \n",
       "2                          596  ...     True    False    False    False   \n",
       "3                          596  ...    False    False    False    False   \n",
       "4                          596  ...    False    False    False    False   \n",
       "...                        ...  ...      ...      ...      ...      ...   \n",
       "8867119                    170  ...    False    False    False     True   \n",
       "8867120                    170  ...     True    False    False    False   \n",
       "8867121                    170  ...    False    False    False    False   \n",
       "8867122                    170  ...     True    False    False    False   \n",
       "8867123                    205  ...    False    False    False     True   \n",
       "\n",
       "         month_7  month_8  month_9  month_10  month_11  month_12  \n",
       "0           True    False    False     False     False     False  \n",
       "1           True    False    False     False     False     False  \n",
       "2          False    False    False     False     False     False  \n",
       "3          False    False    False     False     False     False  \n",
       "4          False    False    False     False     False     False  \n",
       "...          ...      ...      ...       ...       ...       ...  \n",
       "8867119    False    False    False     False     False     False  \n",
       "8867120    False    False    False     False     False     False  \n",
       "8867121    False    False    False     False     False     False  \n",
       "8867122    False    False    False     False     False     False  \n",
       "8867123    False    False    False     False     False     False  \n",
       "\n",
       "[8867124 rows x 88 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_data=pd.read_csv('data_sample/final_data.csv')\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition d'une fonction pour encoder une colonne avec les fréquences\n",
    "def frequency_encode(df, column):\n",
    "    # Calcul de la fréquence relative pour chaque modalité de la colonne\n",
    "    freq_map = df[column].value_counts(normalize=True)  # Renvoie un dictionnaire {valeur: fréquence}\n",
    "    # Remplacement des valeurs de la colonne par leurs fréquences\n",
    "    return df[column].map(freq_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Calcul de la fréquence d'achat des produits par client\n",
    "# Le DataFrame `frequency_df` est créé en regroupant par `customer_id` et `product_id`.\n",
    "# La fréquence est calculée en comptant le nombre de transactions (`transaction_id`) pour chaque paire client-produit.\n",
    "# La méthode `reset_index` renomme automatiquement la colonne résultante en \"frequency\".\n",
    "frequency_df = final_data.groupby(['customer_id', 'product_id'])['transaction_id'].count().reset_index(name='frequency')\n",
    "\n",
    "# Étape 2 : Fusion des fréquences calculées avec le DataFrame d'origine\n",
    "# La colonne `frequency` est ajoutée à `final_data` en utilisant une jointure sur les colonnes `customer_id` et `product_id`.\n",
    "# La jointure est une \"left join\" pour conserver toutes les lignes existantes dans `final_data`.\n",
    "final_data = final_data.merge(frequency_df, on=['customer_id', 'product_id'], how='left')\n",
    "\n",
    "# Étape 3 : Encodage par fréquence pour la colonne `favorite_product`\n",
    "# La fonction `frequency_encode` est appliquée à la colonne `favorite_product` pour la transformer.\n",
    "# Chaque modalité de la colonne `favorite_product` est remplacée par sa fréquence relative dans l'ensemble des données.\n",
    "# Cela permet de capturer l'importance relative des produits préférés des clients.\n",
    "final_data['favorite_product'] = frequency_encode(final_data, 'favorite_product')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_data(final_data):\n",
    "    \"\"\"\n",
    "    Ajoute de nouvelles variables pertinentes au DataFrame final_data.\n",
    "\n",
    "    Paramètre :\n",
    "        final_data (pd.DataFrame) : Le DataFrame de données initiales.\n",
    "\n",
    "    Retourne :\n",
    "        pd.DataFrame : Le DataFrame enrichi avec les nouvelles variables.\n",
    "    \"\"\"\n",
    "    # Conversion explicite de la colonne 'date' au format datetime\n",
    "    # Cela garantit que la colonne 'date' est correctement formatée pour effectuer des calculs de date.\n",
    "    if not pd.api.types.is_datetime64_any_dtype(final_data['date']):\n",
    "        final_data['date'] = pd.to_datetime(final_data['date'], errors='coerce')\n",
    "\n",
    "    # Calcul de la popularité locale des produits par magasin\n",
    "    # Compte le nombre de transactions (transaction_id) par magasin et produit pour mesurer leur popularité locale.\n",
    "    product_popularity_by_store = final_data.groupby(['store_id', 'product_id'])['transaction_id'].count().reset_index()\n",
    "    product_popularity_by_store.rename(columns={'transaction_id': 'local_popularity'}, inplace=True)\n",
    "    final_data = final_data.merge(product_popularity_by_store, on=['store_id', 'product_id'], how='left', suffixes=('', '_drop'))\n",
    "\n",
    "    # Éviter les doublons de colonnes après la jointure\n",
    "    # Supprime les colonnes inutiles créées lors de la jointure.\n",
    "    final_data.drop([col for col in final_data.columns if col.endswith('_drop')], axis=1, inplace=True)\n",
    "\n",
    "    # Calcul du ratio de fréquence globale\n",
    "    # Fréquence locale divisée par la popularité globale du produit.\n",
    "    final_data['frequency_global_ratio'] = final_data['frequency'] / final_data['global_popularity']\n",
    "\n",
    "    # Calcul de la popularité d’un magasin\n",
    "    # Somme des transactions par magasin pour mesurer son activité globale.\n",
    "    store_popularity = final_data.groupby('store_id')['transaction_id'].count()\n",
    "    final_data['store_popularity'] = final_data['store_id'].map(store_popularity)\n",
    "\n",
    "    # Calcul de la variance des achats mensuels par client\n",
    "    # Permet de mesurer la régularité des achats mensuels de chaque client.\n",
    "    monthly_purchases = final_data.groupby(['customer_id', final_data['date'].dt.month])['transaction_id'].count().unstack(fill_value=0)\n",
    "    final_data['monthly_purchase_variance'] = final_data['customer_id'].map(monthly_purchases.var(axis=1))\n",
    "\n",
    "    # Calcul du ratio des produits achetés plusieurs fois par client\n",
    "    # Mesure la fidélité d’un client à certains produits.\n",
    "    repeat_products = final_data.groupby(['customer_id', 'product_id'])['transaction_id'].count().gt(1).groupby('customer_id').mean()\n",
    "    final_data['repeat_product_ratio'] = final_data['customer_id'].map(repeat_products)\n",
    "\n",
    "    # Calcul du temps moyen entre deux transactions pour chaque client\n",
    "    # Mesure la fréquence des transactions des clients.\n",
    "    time_diff = final_data.groupby('customer_id')['date'].diff().dt.days\n",
    "    final_data['avg_time_between_transactions'] = final_data['customer_id'].map(time_diff.groupby(final_data['customer_id']).mean())\n",
    "\n",
    "    # Calcul de la diversité des catégories achetées par client\n",
    "    # Mesure la variété des produits achetés par catégorie.\n",
    "    category_diversity = final_data.groupby('customer_id')['shelf_level1'].nunique()\n",
    "    final_data['category_diversity'] = final_data['customer_id'].map(category_diversity)\n",
    "\n",
    "    # Calcul du ratio de fidélité à une marque par client\n",
    "    # Ratio entre le nombre de marques uniques achetées et le total des marques achetées.\n",
    "    brand_diversity = final_data.groupby('customer_id')['brand_key'].nunique()\n",
    "    total_brands = final_data.groupby('customer_id')['brand_key'].count()\n",
    "    final_data['brand_loyalty_ratio'] = final_data['customer_id'].map(brand_diversity / total_brands)\n",
    "\n",
    "    # Calcul des ratios pour les produits bio, vegan, et sans gluten\n",
    "    # Moyenne des indicateurs booléens pour chaque client.\n",
    "    final_data['bio_ratio'] = final_data.groupby('customer_id')['bio'].transform('mean')\n",
    "    final_data['vegan_ratio'] = final_data.groupby('customer_id')['vegan'].transform('mean')\n",
    "    final_data['gluten_free_ratio'] = final_data.groupby('customer_id')['gluten_free'].transform('mean')\n",
    "\n",
    "    # Calcul du ratio de produits frais par client\n",
    "    # Moyenne des produits frais achetés.\n",
    "    fresh_products = final_data.groupby('customer_id')['fresh'].mean()\n",
    "    final_data['fresh_product_ratio'] = final_data['customer_id'].map(fresh_products)\n",
    "\n",
    "    # Calcul du ratio de produits locaux par client\n",
    "    # Moyenne des produits locaux achetés.\n",
    "    local_ratio = final_data.groupby('customer_id')['local_french'].mean()\n",
    "    final_data['local_product_ratio'] = final_data['customer_id'].map(local_ratio)\n",
    "\n",
    "    # Normalisation de la quantité achetée pour une échelle comparable\n",
    "    # Permet de mettre à l'échelle la variable `quantity`.\n",
    "    final_data['quantity_norm'] = (final_data['quantity'] - final_data['quantity'].min()) / (final_data['quantity'].max() - final_data['quantity'].min())\n",
    "\n",
    "    # Calcul de l'indice de sophistication\n",
    "    # Combinaison pondérée de la quantité et d'une variable binaire liée aux produits de grande consommation (PGC).\n",
    "    final_data['sophistication_index'] = (\n",
    "        0.7 * final_data['sector_PGC'] +       # Poids pour les produits PGC\n",
    "        0.3 * final_data['quantity_norm']      # Poids pour la quantité normalisée\n",
    "    )\n",
    "\n",
    "    return final_data\n",
    "final_data = enhance_data(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Nouveau_dossier\\envs\\visualisation\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Appliquer le logarithme aux variables pertinentes\n",
    "def apply_log_transformations(data):\n",
    "    \"\"\"\n",
    "    Applique des transformations logarithmiques aux variables continues sélectionnées\n",
    "    pour réduire l'impact des valeurs extrêmes et améliorer la distribution.\n",
    "\n",
    "    Paramètres :\n",
    "        data (pd.DataFrame) : Le DataFrame contenant les données à transformer.\n",
    "\n",
    "    Retourne :\n",
    "        pd.DataFrame : Le DataFrame enrichi avec les colonnes transformées logarithmiquement.\n",
    "    \"\"\"\n",
    "    # Variables candidates pour log-transformations\n",
    "    vars_to_log = [\n",
    "        'frequency',  # Fréquence d'achat par client pour un produit\n",
    "        'global_popularity',  # Popularité globale d'un produit\n",
    "        'store_popularity',  # Popularité d'un magasin\n",
    "        'avg_time_between_transactions',  # Temps moyen entre transactions d'un client\n",
    "        'frequency_global_ratio',  # Ratio fréquence d'achat/popularité globale\n",
    "        'repeat_product_ratio',  # Ratio de produits récurrents achetés par client\n",
    "        'category_diversity',  # Diversité des catégories achetées par client\n",
    "        'sophistication_index'  # Indice de sophistication du client basé sur ses achats\n",
    "    ]\n",
    "    \n",
    "    # Vérifier les valeurs non nulles pour éviter log(0)\n",
    "    for var in vars_to_log:\n",
    "        if var in data.columns:  # Vérifie si la colonne existe dans le DataFrame\n",
    "            # Applique la transformation log(1 + x) pour éviter les problèmes avec log(0)\n",
    "            data[f'log_{var}'] = np.log1p(data[var])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Appliquer la transformation sur les données\n",
    "final_data = apply_log_transformations(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des variables d'origine correspondant aux variables logarithmiques\n",
    "vars_to_drop = [\n",
    "    'frequency',\n",
    "    'global_popularity',\n",
    "    'store_popularity',\n",
    "    'avg_time_between_transactions',\n",
    "    'frequency_global_ratio',\n",
    "    'repeat_product_ratio',\n",
    "    'category_diversity',\n",
    "    'sophistication_index'\n",
    "]\n",
    "\n",
    "# Supprimer ces variables\n",
    "final_data = final_data.drop(columns=vars_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les variable utilisée pour notre meilleurs modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apres des etudes , nous avons selectionner ces variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quantity',\n",
       " 'total_transactions',\n",
       " 'unique_buyers',\n",
       " 'store_id',\n",
       " 'class_key',\n",
       " 'subclass_key',\n",
       " 'brand_key',\n",
       " 'shelf_level1',\n",
       " 'shelf_level2',\n",
       " 'shelf_level3',\n",
       " 'purchase_frequency_x',\n",
       " 'favorite_product',\n",
       " 'avg_products_per_transaction',\n",
       " 'promo_sensitivity_x',\n",
       " 'last_product',\n",
       " 'promo_sensitivity_y',\n",
       " 'loyalty_card_usage',\n",
       " 'total_unique_products',\n",
       " 'time_since_last_purchase',\n",
       " 'local_popularity',\n",
       " 'monthly_purchase_variance',\n",
       " 'quantity_norm',\n",
       " 'brand_loyalty_ratio',\n",
       " 'bio_ratio',\n",
       " 'vegan_ratio',\n",
       " 'gluten_free_ratio',\n",
       " 'fresh_product_ratio',\n",
       " 'local_product_ratio',\n",
       " 'log_frequency',\n",
       " 'log_store_popularity',\n",
       " 'log_global_popularity',\n",
       " 'log_avg_time_between_transactions',\n",
       " 'log_frequency_global_ratio',\n",
       " 'log_repeat_product_ratio',\n",
       " 'log_category_diversity',\n",
       " 'log_sophistication_index']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liste des variables originales\n",
    "select_original = [\n",
    "    'frequency', 'quantity', 'total_transactions',\n",
    "    'global_popularity', 'unique_buyers',\n",
    "    'store_id', 'class_key', 'subclass_key', 'brand_key',\n",
    "    'shelf_level1', 'shelf_level2', 'shelf_level3', 'purchase_frequency_x', 'favorite_product',\n",
    "    'avg_products_per_transaction', 'promo_sensitivity_x', 'last_product',\n",
    "    'promo_sensitivity_y', 'loyalty_card_usage', 'total_unique_products', 'time_since_last_purchase',\n",
    "    \"local_popularity\", \"frequency_global_ratio\", \"store_popularity\", \"monthly_purchase_variance\",\n",
    "    \"repeat_product_ratio\", \"avg_time_between_transactions\", 'sophistication_index', 'quantity_norm',\n",
    "    \"category_diversity\", \"brand_loyalty_ratio\", \"bio_ratio\", \"vegan_ratio\", \n",
    "    \"gluten_free_ratio\", \"fresh_product_ratio\", \"local_product_ratio\"\n",
    "]\n",
    "\n",
    "# Liste des variables logarithmiques\n",
    "select_log = [\n",
    "    'log_frequency', 'log_store_popularity','log_global_popularity',\n",
    "    'log_avg_time_between_transactions', 'log_frequency_global_ratio',\n",
    "    'log_repeat_product_ratio', 'log_category_diversity', 'log_sophistication_index'\n",
    "]\n",
    "\n",
    "# Créer une liste unique en prenant les logs si disponibles\n",
    "select = [\n",
    "    var for var in select_original if var not in [\n",
    "        'frequency', 'global_popularity', 'store_popularity',\n",
    "        'avg_time_between_transactions', 'frequency_global_ratio',\n",
    "        'repeat_product_ratio', 'category_diversity', 'sophistication_index'\n",
    "    ]\n",
    "] + select_log\n",
    "\n",
    "# Afficher la liste finale\n",
    "select\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Division du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mise à jour des variables explicatives\n",
    "X = final_data[select]\n",
    "y = final_data['purchased_next']\n",
    "\n",
    "# Diviser en train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement du modele LGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apres le RandomizedSearchCV nous avons selectionner ces meilleurs hyperparametre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 825027, number of negative: 6268672\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.363734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7573\n",
      "[LightGBM] [Info] Number of data points in the train set: 7093699, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116304 -> initscore=-2.027904\n",
      "[LightGBM] [Info] Start training from score -2.027904\n",
      "LGBoost - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97   1567168\n",
      "           1       0.98      0.50      0.66    206257\n",
      "\n",
      "    accuracy                           0.94   1773425\n",
      "   macro avg       0.96      0.75      0.82   1773425\n",
      "weighted avg       0.94      0.94      0.93   1773425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Paramètres fixes pour LightGBM\n",
    "lgb_params = {\n",
    "    'n_estimators': 2000,      # Nombre d'arbres\n",
    "    'learning_rate': 0.1,      # Taux d'apprentissage\n",
    "    'num_leaves': 60,          # Nombre de feuilles\n",
    "    'max_depth': -1,           # Profondeur maximale des arbres\n",
    "    'subsample': 0.8,          # Sous-échantillonnage des données\n",
    "    'colsample_bytree': 1.0,   # Sous-échantillonnage des colonnes\n",
    "    'boosting_type': 'gbdt',   # Type de boosting\n",
    "    'n_jobs': -1,              # Utilisation maximale des cœurs de CPU\n",
    "    'random_state': 42         # Reproductibilité des résultats\n",
    "}\n",
    "\n",
    "# Modèle LightGBM avec les paramètres définis\n",
    "print('Entraînement du modèle LightGBM...')\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "# Entraînement du modèle\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "lgb_predictions = lgb_model.predict(X_test)\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"LGBoost - Classification Report:\\n\", classification_report(y_test, lgb_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde du modele "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgb_best2.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(lgb_model,'lgb_best2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "best_lgb=load('lgb_best2.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ` LA PREDICTION `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la prediction , j'effecture les meme transformation sur un jeu de donnees train_data afin d'avoir une idee sur hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14712\\2033622296.py:3: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  products_data = pd.read_csv('data-train/products_data.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "final_data1=pd.read_csv('data-train/train_data_part_5.csv')\n",
    "products_data = pd.read_csv('data-train/products_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "products_data=products_data.drop(columns=['shelf_level4','ecoscore','salt_reduced','naturality',\n",
    "                                          'product_description','lactose_free','no_added_salt'])\n",
    "\n",
    "#Imputer les valeur manquantes\n",
    "import numpy as np\n",
    "\n",
    "def Imput_ValeurManquate ( Data, columns_Num, columns_Cat, Imput_Num= \"median\", \n",
    "                          Imput_Cat= \"Mode\" ) :\n",
    "    for col in columns_Num :\n",
    "        if Imput_Num == \"median\" :\n",
    "            Change = np.median(Data[col].dropna()) \n",
    "        else:  \n",
    "            Change = int(Imput_Num)\n",
    "        Data[col] = Data[col].fillna(Change)\n",
    "    for col in columns_Cat :\n",
    "        if Imput_Cat == \"Mode\" :\n",
    "            Change = str(Data[col].mode()[0])\n",
    "        else:  \n",
    "            Change = str(Imput_Cat)\n",
    "        Data[col] = Data[col].fillna(Change)\n",
    "    return(Data)    \n",
    "    \n",
    "\n",
    "products_data = Imput_ValeurManquate(products_data, [\"alcool\"], ['brand_key','shelf_level3'])\n",
    "\n",
    "# Fusion des données sur product_id\n",
    "final_data1 = pd.merge(final_data1, products_data, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_df1 = final_data1.groupby(['customer_id', 'product_id'])['transaction_id'].count().reset_index(name='frequency')\n",
    "# Ajouter la colonne fréquence au DataFrame d'origine\n",
    "final_data1 =final_data1.merge(frequency_df1, on=['customer_id', 'product_id'], how='left')\n",
    "final_data1['total_transactions'] = final_data1.groupby('customer_id')['transaction_id'].transform('count')\n",
    "final_data1['total_unique_products'] = final_data1.groupby('customer_id')['product_id'].transform('nunique')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data1['date'] = pd.to_datetime(final_data1['date'])\n",
    "final_data1['day_of_week'] = final_data1['date'].dt.dayofweek\n",
    "final_data1['month'] = final_data1['date'].dt.month\n",
    "\n",
    "customer_frequency = final_data1.groupby('customer_id')['transaction_id'].nunique().reset_index()\n",
    "customer_frequency.rename(columns={'transaction_id': 'purchase_frequency'}, inplace=True)\n",
    "final_data1 = final_data1.merge(customer_frequency, on='customer_id', how='left')\n",
    "\n",
    "favorite_products = final_data1.groupby(['customer_id', 'product_id'])['quantity'].sum().reset_index()\n",
    "favorite_products = favorite_products.sort_values(['customer_id', 'quantity'], ascending=[True, False]).groupby('customer_id').head(1)\n",
    "favorite_products.rename(columns={'product_id': 'favorite_product'}, inplace=True)\n",
    "final_data1 = final_data1.merge(favorite_products[['customer_id', 'favorite_product']], on='customer_id', how='left')\n",
    "\n",
    "\n",
    "customer_avg_products = final_data1.groupby('customer_id')['quantity'].mean().reset_index()\n",
    "customer_avg_products.rename(columns={'quantity': 'avg_products_per_transaction'}, inplace=True)\n",
    "\n",
    "customer_promo_sensitivity = final_data1.groupby('customer_id')['is_promo'].mean().reset_index()\n",
    "customer_promo_sensitivity.rename(columns={'is_promo': 'promo_sensitivity'}, inplace=True)\n",
    "\n",
    "last_purchase = final_data1.sort_values('date').groupby('customer_id')['product_id'].last().reset_index()\n",
    "last_purchase.rename(columns={'product_id': 'last_product'}, inplace=True)\n",
    "\n",
    "favorite_category = final_data1.groupby(['customer_id', 'department_key'])['quantity'].sum().reset_index()\n",
    "favorite_category = favorite_category.sort_values(['customer_id', 'quantity'], ascending=[True, False]).groupby('customer_id').head(1)\n",
    "favorite_category.rename(columns={'department_key': 'favorite_category'}, inplace=True)\n",
    "\n",
    "\n",
    "product_popularity = final_data1.groupby('product_id')['quantity'].sum().reset_index()\n",
    "product_popularity.rename(columns={'quantity': 'global_popularity'}, inplace=True)\n",
    "\n",
    "\n",
    "product_promo_sensitivity = final_data1.groupby('product_id')['is_promo'].mean().reset_index()\n",
    "product_promo_sensitivity.rename(columns={'is_promo': 'promo_sensitivity'}, inplace=True)\n",
    "\n",
    "\n",
    "product_unique_buyers = final_data1.groupby('product_id')['customer_id'].nunique().reset_index()\n",
    "product_unique_buyers.rename(columns={'customer_id': 'unique_buyers'}, inplace=True)\n",
    "\n",
    "\n",
    "# Exemple de fusion pour toutes les features clients\n",
    "#final_data = train_data.copy()\n",
    "final_data1 = final_data1.merge(customer_frequency, on='customer_id', how='left')\n",
    "final_data1 = final_data1.merge(customer_avg_products, on='customer_id', how='left')\n",
    "final_data1 = final_data1.merge(customer_promo_sensitivity, on='customer_id', how='left')\n",
    "final_data1 = final_data1.merge(last_purchase, on='customer_id', how='left')\n",
    "#final_data = final_data.merge(favorite_products[['customer_id', 'favorite_product']], on='customer_id', how='left')\n",
    "#final_data = final_data.merge(favorite_category[['customer_id', 'favorite_category']], on='customer_id', how='left')\n",
    "\n",
    "\n",
    "# Exemple de fusion pour toutes les features produits\n",
    "final_data1 = final_data1.merge(product_popularity, on='product_id', how='left')\n",
    "final_data1 = final_data1.merge(product_promo_sensitivity, on='product_id', how='left')\n",
    "final_data1 = final_data1.merge(product_unique_buyers, on='product_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_target = ['class_key','shelf_level1','store_id', 'subclass_key', 'brand_key','shelf_level2', 'shelf_level3']  # Colonnes importantes pour Target Encoding\n",
    "\n",
    "# Fonction pour encoder avec les fréquences\n",
    "def frequency_encode(df, column):\n",
    "    freq_map = df[column].value_counts(normalize=True)  # Fréquences\n",
    "    return df[column].map(freq_map)\n",
    "\n",
    "# Encoder chaque colonne catégorique avec les fréquences\n",
    "for col in categorical_cols_target:\n",
    "    final_data1[col] = frequency_encode(final_data1, col)\n",
    "\n",
    "#final_data=final_data.drop(columns=['purchase_frequency_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modalités à garder\n",
    "valid_modalities = {\n",
    "    'format': ['DRIVE', 'CLCV'],\n",
    "    'order_channel': ['WEBSITE', 'MOBILE_APP'],\n",
    "    'department_key': ['Department_25', 'Department_14', 'Department_10', 'Department_22', 'Department_12'],\n",
    "    'sector': ['PGC', 'PRODUITS FRAIS TRANSFORMATION']\n",
    "}\n",
    "\n",
    "# Filtrer les colonnes pour ne garder que les modalités pertinentes\n",
    "for column, modalities in valid_modalities.items():\n",
    "    final_data1[column] = final_data1[column].apply(lambda x: x if x in modalities else None)\n",
    "\n",
    "# Encoder avec get_dummies\n",
    "final_data1 = pd.get_dummies(final_data1, columns=valid_modalities.keys(), drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifiez les colonnes catégoriques et booléennes\n",
    "categorical_cols_onehot = ['day_of_week', 'month']\n",
    "# Step 1: One-Hot Encoding pour les colonnes catégoriques avec peu de modalités\n",
    "final_data1 = pd.get_dummies(final_data1, columns=categorical_cols_onehot, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data1['favorite_product'] = frequency_encode(final_data1, 'favorite_product')\n",
    "final_data1['last_product'] = frequency_encode(final_data1, 'last_product')\n",
    "final_data1['favorite_product']=final_data1['favorite_product'].astype('float64')\n",
    "final_data1['loyalty_card_usage'] = final_data1.groupby('customer_id')['has_loyality_card'].transform('mean')\n",
    "#train_data['purchase_frequency'] = train_data.groupby('customer_id')['transaction_id'].transform(lambda x: x.nunique() / 2)  # assuming 2 years of data\n",
    "final_data1['time_since_last_purchase'] = final_data1.groupby('customer_id')['date'].transform(lambda x: (pd.to_datetime(x).max() - pd.to_datetime(x)).dt.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_data(final_data):\n",
    "    \"\"\"\n",
    "    Ajoute de nouvelles variables pertinentes au DataFrame final_data.\n",
    "    \n",
    "    Paramètre :\n",
    "        final_data (pd.DataFrame) : Le DataFrame de données initiales.\n",
    "    \n",
    "    Retourne :\n",
    "        pd.DataFrame : Le DataFrame enrichi avec les nouvelles variables.\n",
    "    \"\"\"\n",
    "    # Conversion explicite de la colonne 'date' au format datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(final_data['date']):\n",
    "        final_data['date'] = pd.to_datetime(final_data['date'], errors='coerce')\n",
    "\n",
    "    # Calcul de la popularité locale des produits par magasin\n",
    "    product_popularity_by_store = final_data.groupby(['store_id', 'product_id'])['transaction_id'].count().reset_index()\n",
    "    product_popularity_by_store.rename(columns={'transaction_id': 'local_popularity'}, inplace=True)\n",
    "    final_data = final_data.merge(product_popularity_by_store, on=['store_id', 'product_id'], how='left', suffixes=('', '_drop'))\n",
    "    \n",
    "    # Éviter les doublons de colonnes après le merge\n",
    "    final_data.drop([col for col in final_data.columns if col.endswith('_drop')], axis=1, inplace=True)\n",
    "\n",
    "    # Calcul du ratio de fréquence globale\n",
    "    final_data['frequency_global_ratio'] = final_data['frequency'] / final_data['global_popularity']\n",
    "\n",
    "    # Calcul de la popularité d’un magasin\n",
    "    store_popularity = final_data.groupby('store_id')['transaction_id'].count()\n",
    "    final_data['store_popularity'] = final_data['store_id'].map(store_popularity)\n",
    "\n",
    "    # Calcul de la variance des achats mensuels par client\n",
    "    monthly_purchases = final_data.groupby(['customer_id', final_data['date'].dt.month])['transaction_id'].count().unstack(fill_value=0)\n",
    "    final_data['monthly_purchase_variance'] = final_data['customer_id'].map(monthly_purchases.var(axis=1))\n",
    "\n",
    "    # Calcul du ratio des produits achetés plusieurs fois par client\n",
    "    repeat_products = final_data.groupby(['customer_id', 'product_id'])['transaction_id'].count().gt(1).groupby('customer_id').mean()\n",
    "    final_data['repeat_product_ratio'] = final_data['customer_id'].map(repeat_products)\n",
    "\n",
    "    # Calcul du temps moyen entre deux transactions pour chaque client\n",
    "    time_diff = final_data.groupby('customer_id')['date'].diff().dt.days\n",
    "    final_data['avg_time_between_transactions'] = final_data['customer_id'].map(time_diff.groupby(final_data['customer_id']).mean())\n",
    "\n",
    "    # Calcul de la diversité des catégories achetées par client\n",
    "    category_diversity = final_data.groupby('customer_id')['shelf_level1'].nunique()\n",
    "    final_data['category_diversity'] = final_data['customer_id'].map(category_diversity)\n",
    "\n",
    "    # Calcul du ratio de fidélité à une marque par client\n",
    "    brand_diversity = final_data.groupby('customer_id')['brand_key'].nunique()\n",
    "    total_brands = final_data.groupby('customer_id')['brand_key'].count()\n",
    "    final_data['brand_loyalty_ratio'] = final_data['customer_id'].map(brand_diversity / total_brands)\n",
    "\n",
    "    # Calcul des ratios pour les produits bio, vegan, et sans gluten\n",
    "    final_data['bio_ratio'] = final_data.groupby('customer_id')['bio'].transform('mean')\n",
    "    final_data['vegan_ratio'] = final_data.groupby('customer_id')['vegan'].transform('mean')\n",
    "    final_data['gluten_free_ratio'] = final_data.groupby('customer_id')['gluten_free'].transform('mean')\n",
    "\n",
    "    # Calcul du ratio de produits frais par client\n",
    "    fresh_products = final_data.groupby('customer_id')['fresh'].mean()\n",
    "    final_data['fresh_product_ratio'] = final_data['customer_id'].map(fresh_products)\n",
    "\n",
    "    # Calcul du ratio de produits locaux par client\n",
    "    local_ratio = final_data.groupby('customer_id')['local_french'].mean()\n",
    "    final_data['local_product_ratio'] = final_data['customer_id'].map(local_ratio)\n",
    "\n",
    "    # Normalisation de la quantité achetée pour une échelle comparable\n",
    "    final_data['quantity_norm'] = (final_data['quantity'] - final_data['quantity'].min()) / (final_data['quantity'].max() - final_data['quantity'].min())\n",
    "\n",
    "    # Calcul de l'indice de sophistication en combinant sector_PGC et quantity avec pondération\n",
    "    final_data['sophistication_index'] = (\n",
    "        0.7 * final_data['sector_PGC'] +       # Poids pour la variable sector_PGC (produits PGC)\n",
    "        0.3 * final_data['quantity_norm']      # Poids plus élevé pour la quantité achetée\n",
    "    )\n",
    "\n",
    "    return final_data\n",
    "final_data1 = enhance_data(final_data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Nouveau_dossier\\envs\\visualisation\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\HP\\anaconda3\\Nouveau_dossier\\envs\\visualisation\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Appliquer le logarithme aux variables pertinentes\n",
    "def apply_log_transformations(data):\n",
    "    # Variables candidates pour log-transformations\n",
    "    vars_to_log = [\n",
    "        'frequency',\n",
    "        'global_popularity',\n",
    "        'store_popularity',\n",
    "        'avg_time_between_transactions',\n",
    "        'frequency_global_ratio',\n",
    "        'repeat_product_ratio',\n",
    "        'category_diversity',\n",
    "        'sophistication_index'\n",
    "    ]\n",
    "    \n",
    "    # Vérifier les valeurs non nulles pour éviter log(0)\n",
    "    for var in vars_to_log:\n",
    "        if var in data.columns:\n",
    "            data[f'log_{var}'] = np.log1p(data[var])  # log1p(x) = log(1 + x), évite log(0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Appliquer la transformation sur les données\n",
    "final_data1 = apply_log_transformations(final_data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des variables d'origine correspondant aux variables logarithmiques\n",
    "vars_to_drop = [\n",
    "    'frequency',\n",
    "    'global_popularity',\n",
    "    'store_popularity',\n",
    "    'avg_time_between_transactions',\n",
    "    'frequency_global_ratio',\n",
    "    'repeat_product_ratio',\n",
    "    'category_diversity',\n",
    "    'sophistication_index'\n",
    "]\n",
    "\n",
    "# Supprimer ces variables\n",
    "final_data1 = final_data1.drop(columns=vars_to_drop, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des variables originales\n",
    "select_original = [\n",
    "    'frequency', 'quantity', 'total_transactions',\n",
    "    'global_popularity', 'unique_buyers',\n",
    "    'store_id', 'class_key', 'subclass_key', 'brand_key',\n",
    "    'shelf_level1', 'shelf_level2', 'shelf_level3', 'purchase_frequency_x', 'favorite_product',\n",
    "    'avg_products_per_transaction', 'promo_sensitivity_x', 'last_product',\n",
    "    'promo_sensitivity_y', 'loyalty_card_usage', 'total_unique_products', 'time_since_last_purchase',\n",
    "    \"local_popularity\", \"frequency_global_ratio\", \"store_popularity\", \"monthly_purchase_variance\",\n",
    "    \"repeat_product_ratio\", \"avg_time_between_transactions\", 'sophistication_index', 'quantity_norm',\n",
    "    \"category_diversity\", \"brand_loyalty_ratio\", \"bio_ratio\", \"vegan_ratio\", \n",
    "    \"gluten_free_ratio\", \"fresh_product_ratio\", \"local_product_ratio\"\n",
    "]\n",
    "\n",
    "# Liste des variables logarithmiques\n",
    "select_log = [\n",
    "    'log_frequency', 'log_store_popularity','log_global_popularity',\n",
    "    'log_avg_time_between_transactions', 'log_frequency_global_ratio',\n",
    "    'log_repeat_product_ratio', 'log_category_diversity', 'log_sophistication_index'\n",
    "]\n",
    "\n",
    "# Créer une liste unique en prenant les logs si disponibles\n",
    "select = [\n",
    "    var for var in select_original if var not in [\n",
    "        'frequency', 'global_popularity', 'store_popularity',\n",
    "        'avg_time_between_transactions', 'frequency_global_ratio',\n",
    "        'repeat_product_ratio', 'category_diversity', 'sophistication_index'\n",
    "    ]\n",
    "] + select_log\n",
    "\n",
    "# Afficher la liste finale\n",
    "#select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=final_data1[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions des probabilités d'achat\n",
    "recommendations_df =final_data1\n",
    "probabilities = lgb_model.predict_proba(X1)[:, 1]  # Prendre la probabilité de classe positive\n",
    "\n",
    "# Ajouter les probabilités au DataFrame\n",
    "recommendations_df['purchase_probability'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14712\\881829423.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_recommendations['rank'] = unique_recommendations.groupby('customer_id')['purchase_probability'].rank(method='first', ascending=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             customer_id     product_id  purchase_probability  rank\n",
      "578741   Household_40001  Product_65763              0.813827   1.0\n",
      "5091343  Household_40001  Product_28633              0.790623   2.0\n",
      "6412021  Household_40001  Product_47398              0.739364   3.0\n",
      "4300600  Household_40001  Product_67559              0.638802   4.0\n",
      "838820   Household_40001  Product_65000              0.519618   5.0\n",
      "...                  ...            ...                   ...   ...\n",
      "5223393  Household_50000  Product_70198              0.404786   6.0\n",
      "4698150  Household_50000  Product_36150              0.323159   7.0\n",
      "5488076  Household_50000  Product_70866              0.266328   8.0\n",
      "2835274  Household_50000  Product_35141              0.233588   9.0\n",
      "1248787  Household_50000  Product_54454              0.220358  10.0\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Étape 1: Trier le DataFrame par 'customer_id' et 'purchase_probability'\n",
    "sorted_recommendations = recommendations_df.sort_values(by=['customer_id', 'purchase_probability'], ascending=[True, False])\n",
    "\n",
    "# Étape 2: Supprimer les doublons\n",
    "# Garder uniquement la première occurrence de chaque produit pour chaque client\n",
    "unique_recommendations = sorted_recommendations.drop_duplicates(subset=['customer_id', 'product_id'])\n",
    "\n",
    "\n",
    "# Étape 3: Ajouter le rang basé sur la probabilité d'achat\n",
    "unique_recommendations['rank'] = unique_recommendations.groupby('customer_id')['purchase_probability'].rank(method='first', ascending=False)\n",
    "\n",
    "# Étape 4: Sélectionner les 10 meilleurs produits pour chaque client\n",
    "top_recommendations = unique_recommendations[unique_recommendations['rank'] <= 10]\n",
    "\n",
    "# Afficher les recommandations finales avec le rang\n",
    "print(top_recommendations[['customer_id', 'product_id', 'purchase_probability', 'rank']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les recommandations finales avec le rang\n",
    "top_recommendations=top_recommendations[['customer_id', 'product_id','rank']]\n",
    "top_recommendations['rank'] = top_recommendations['rank'].astype(int)\n",
    "#top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitrate@10 evaluation function\n",
    "\n",
    "def hitrate_at_k(true_data: pd.DataFrame,\n",
    "                 predicted_data: pd.DataFrame,\n",
    "                 k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the hitrate at k for the recommendations.\n",
    "    It assesses how relevant our 10 product recommendations are.\n",
    "    In other words, it calculates the proportion of recommended products that are actually purchased by the customer.\n",
    "\n",
    "    Args:\n",
    "        true_data: a pandas DataFrame containing the true data\n",
    "            customer_id: the customer identifier\n",
    "            product_id: the product identifier that was purchased in the test set\n",
    "        predicted_data: a pandas DataFrame containing the predicted data\n",
    "            customer_id: the customer identifier\n",
    "            product_id: the product identifier that was recommended\n",
    "            rank: the rank of the recommendation. the rank should be between 1 and 10.\n",
    "        k: the number of recommendations to consider. k should be between 1 and 10.\n",
    "    \n",
    "    Returns:\n",
    "        The hitrate at k\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.merge(left = true_data, right = predicted_data, how = \"left\", on = [\"customer_id\", \"product_id\"])\n",
    "    df = data[data[\"rank\"] <= k]\n",
    "    non_null_counts = df.groupby('customer_id')['rank'].apply(lambda x: x.notna().sum()).reset_index(name='non_null_count')\n",
    "    distinct_products_per_customer = data.groupby('customer_id')['product_id'].nunique().reset_index(name='distinct_product_count')\n",
    "    df = pd.merge(left = distinct_products_per_customer, right = non_null_counts, how = \"left\", on = \"customer_id\")\n",
    "    df[\"denominator\"] = [min(df.iloc[i].distinct_product_count,k) for i in range(len(df))]\n",
    "    df = df.fillna(0)\n",
    "    return (df[\"non_null_count\"]/df[\"denominator\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('data-train/test_data.csv')\n",
    "# Supposons que 'customer_id' est la clé commune\n",
    "common_customers = final_data1['customer_id'].unique()  # Extraire les individus uniques de train_data\n",
    "\n",
    "# Filtrer test_data pour ne garder que les individus communs\n",
    "test_data = test_data[test_data['customer_id'].isin(common_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitrate@10 est 0.3630\n"
     ]
    }
   ],
   "source": [
    "# Calculate the hitrate at k for k = 10\n",
    "model_hitrate_at_10 = hitrate_at_k(test_data,top_recommendations,10)\n",
    "print(f\"Hitrate@10 est {model_hitrate_at_10:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
